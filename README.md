# Demo page of "Toward Universal Speech Enhancement for Diverse Input Conditions" (ASRU 2023)

## Introduction
We devise a single **u**niversal **sp**eech **e**nhancement and **s**eparation (USES) model that is independent of microphone channels, signal lengths, and sampling frequencies. It can perform denoising and dereverberation in diverse input conditions with strong performance. This demo page demonstrates the audios enhanced by the proposed universal SE model and the baseline SE models in five different corpora ([VoiceBank+DEMAND](https://datashare.ed.ac.uk/handle/10283/2791), [DNS1](https://github.com/microsoft/DNS-Challenge/tree/interspeech2020/master), [WHAMR!](https://wham.whisper.ai/), [CHiME-4](https://spandh.dcs.shef.ac.uk/chime_challenge/CHiME4/download.html), and [REVERB](http://reverb2014.dereverberation.com/download.html)).

```bibtex
@inproceedings{Toward-Zhang2023,
  title={Toward Universal Speech Enhancement for Diverse Input Conditions},
  author={Zhang, Wangyou and Saijo, Kohei and Wang, Zhong-Qiu and Watanabe, Shinji and Qian, Yanmin},
  booktitle={Proc. IEEE ASRU},
  year={2023},
}
```


## Acknowledgment
This demo page is built upon [audiolabs/webMUSHRA](https://github.com/audiolabs/webMUSHRA), the listening test framework.
